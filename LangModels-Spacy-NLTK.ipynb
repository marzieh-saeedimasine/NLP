{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a622b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-- Language models: SpaCy and NLTK (Natural Language Toolkit) for sentense and word tokenizing. SpaCy is faster and better for production; NLTK is great for research and education.\n",
    "-- SpaCy pipeline consists of different attributes: tagger(.pos_), parser, lemmatizer(.lemma_), ner(.ent) and dependency (.dep_) read more in https://spacy.io/usage/processing-pipelines#pipelines\n",
    "-- Stemming uses simple rules to drive base word but Lemmatization uses knowledge of language to drive the base word. spacy doesnt has stemming, NLTK has both stemming and lemma\n",
    "-- Difference of POS and Tag: coarse-grained vs. fine-grained category of a word\n",
    "-- Name Entity Recognition (NER) for Person, Company, Product, Location, Money and how to customize entities\n",
    "-- Stop words carry little meaningful information and are ignored in text processing tasks.like articles, prepositions, pronouns, conjunctions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2a9a4ee2-8390-4473-8a7f-b5eadd1aa98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk    \n",
    "# nltk.download('punkt')\n",
    "# !pip install spacy\n",
    "# !python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "473db0bc-1e8f-49f7-9d9a-d5163af2b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import  sent_tokenize, word_tokenize  \n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "from spacy import displacy\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer \n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63c9c9b-ba04-491c-993b-07ece27333f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An end to end NLP project consists of many steps. 11\n",
      "An\n",
      "end\n",
      "to\n",
      "end\n",
      "NLP\n",
      "project\n",
      "consists\n",
      "of\n",
      "many\n",
      "steps\n",
      ".\n",
      "These steps together forms an NLP pipeline. 8\n",
      "These\n",
      "steps\n",
      "together\n",
      "forms\n",
      "an\n",
      "NLP\n",
      "pipeline\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(\"An end to end NLP project consists of many steps. These steps together forms an NLP pipeline.\")\n",
    "\n",
    "for sentense in doc.sents:\n",
    "    print(sentense, len(sentense))\n",
    "    for word in sentense:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aceb42ed-0745-4562-8f8e-25b3b4820ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An end to end NLP project consists of many steps.', 'These steps together forms an NLP pipeline.']\n",
      "['An', 'end', 'to', 'end', 'NLP', 'project', 'consists', 'of', 'many', 'steps', '.', 'These', 'steps', 'together', 'forms', 'an', 'NLP', 'pipeline', '.']\n"
     ]
    }
   ],
   "source": [
    "doc=sent_tokenize(\"An end to end NLP project consists of many steps. These steps together forms an NLP pipeline.\")\n",
    "print(doc)\n",
    "\n",
    "doc=word_tokenize(\"An end to end NLP project consists of many steps. These steps together forms an NLP pipeline.\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33c920-84c1-4a15-b3df-114c0d3d4865",
   "metadata": {},
   "source": [
    "# Make a spacy blank object that just have tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "682b84f7-cb0b-47aa-b729-7b8a7295ee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.blank(\"en\")  #blank lang model just have tokenizer. a blank pipeline need to add different features to it\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3357601c-3a3d-4af9-bfd1-ca3525314c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6240b3d0-a6d5-4142-8ba1-1a5a8faa9e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].text\n",
    "doc[0].is_currency\n",
    "doc[0].is_stop\n",
    "doc[0].i  #token index\n",
    "doc[0].like_num\n",
    "doc[0].is_punct\n",
    "doc[0].is_oov \n",
    "doc[0].like_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfe5b709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc=nlp(text)\n",
    "url_list=[]\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        url_list.append(token.text)\n",
    "\n",
    "url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f0264e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc=nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        if doc[(token.i)+1].is_currency:  \n",
    "            print(token.text, doc[(token.i)+1])       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34474d",
   "metadata": {},
   "source": [
    "# Add components to the blank pipeline uisng add_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f89247e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senter']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_source=spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('senter',source=nlp_source)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15a7b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An end to end NLP project consists of many steps. 11\n",
      "An\n",
      "end\n",
      "to\n",
      "end\n",
      "NLP\n",
      "project\n",
      "consists\n",
      "of\n",
      "many\n",
      "steps\n",
      ".\n",
      "These steps together forms an NLP pipeline. 8\n",
      "These\n",
      "steps\n",
      "together\n",
      "forms\n",
      "an\n",
      "NLP\n",
      "pipeline\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"An end to end NLP project consists of many steps. These steps together forms an NLP pipeline.\")\n",
    "\n",
    "for sentense in doc.sents:\n",
    "    print(sentense, len(sentense))\n",
    "    for word in sentense:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48f9b02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senter', 'ner']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"ner\", source=nlp_source)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f43480f2-29ce-403d-a186-410640455159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span=doc[:4]\n",
    "type(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0625df7-309a-48af-98ad-14e5bfc05d82",
   "metadata": {},
   "source": [
    "# spacy pipeline contains: tagger(.pos_), parser, lemmatizer(.lemma_),ner(.ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "438e17b4-f81b-4a18-826d-61d6a4923fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "nlp.pipe_names\n",
    "#nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0beb68e-9001-4b5f-a2df-5a555ac5a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  PROPN proper noun  |  Captain\n",
      "america  |  PROPN proper noun  |  america\n",
      "ate  |  VERB verb  |  eat\n",
      "100  |  NUM numeral  |  100\n",
      "$  |  NUM numeral  |  $\n",
      "of  |  ADP adposition  |  of\n",
      "samosa  |  PROPN proper noun  |  samosa\n",
      ".  |  PUNCT punctuation  |  .\n",
      "Then  |  ADV adverb  |  then\n",
      "he  |  PRON pronoun  |  he\n",
      "said  |  VERB verb  |  say\n",
      "I  |  PRON pronoun  |  I\n",
      "can  |  AUX auxiliary  |  can\n",
      "do  |  VERB verb  |  do\n",
      "this  |  PRON pronoun  |  this\n",
      "all  |  DET determiner  |  all\n",
      "day  |  NOUN noun  |  day\n",
      ".  |  PUNCT punctuation  |  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "for token in doc:\n",
    "    print(token,\" | \",token.pos_, spacy.explain(token.pos_), \" | \",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff9fb194-c33e-47c1-b89e-82cb8459eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG\n",
      "Twitter  |  PRODUCT\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text ,\" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80b94ee4-6783-424c-8836-d3e8ab01a70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b9352d9-3636-4ba8-bfaa-3aae0a3f84d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1a04e16d82b8459a96cc9b7c2e3d9cf8-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Tesla</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Inc</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">acquire</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">twitter</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">45</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,266.5 L928.0,254.5 912.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1a04e16d82b8459a96cc9b7c2e3d9cf8-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5835fec-f977-453b-b8f1-d06365507738",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc8583b3-f473-4365-84a9-13dacaafc67f",
   "metadata": {},
   "source": [
    "# SpaCy for swedish lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6261f296-0fd2-4e8d-900d-b2d799d40745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download sv_core_news_sm\n",
    "#!python3 -m spacy download sv_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1c81a30-4ad8-427f-81de-5012252831ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'lemmatizer',\n",
       " 'attribute_ruler',\n",
       " 'ner']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"sv_core_news_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05ae82b6-ea10-4a6e-84fe-cfd67ec194bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ADJ adjective  |  tesla\n",
      "Inc  |  NOUN noun  |  Inc\n",
      "kommer  |  AUX auxiliary  |  komma\n",
      "att  |  PART particle  |  att\n",
      "förvärva  |  VERB verb  |  förvärva\n",
      "twitter  |  NOUN noun  |  twitt\n",
      "för  |  ADP adposition  |  för\n",
      "45  |  NUM numeral  |  45\n",
      "miljarder  |  NOUN noun  |  miljard\n",
      "dollar  |  NOUN noun  |  doll\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc kommer att förvärva twitter för 45 miljarder dollar\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \",token.pos_ , spacy.explain(token.pos_),\" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "690e1c33-7484-4ff3-b25a-6e2cd95805b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 miljarder dollar  |  MSR\n"
     ]
    }
   ],
   "source": [
    "nlp_source=spacy.load('sv_core_news_lg')\n",
    "\n",
    "doc = nlp_source(\"Tesla Inc kommer att förvärva twitter för 45 miljarder dollar\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\" | \",ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5155a98",
   "metadata": {},
   "source": [
    "# Customize the NLP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b0cc793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens=[token.text for token in doc]\n",
    "tokens\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[{ORTH:'gim'},{ORTH:'me'}])\n",
    "\n",
    "tokens=[token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc656f",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2a2eab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  ate\n",
      "adjustable  |  adjust\n",
      "rafting  |  raft\n",
      "ability  |  abil\n",
      "meeting  |  meet\n"
     ]
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "\n",
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "for word in words:\n",
    "    print(word, \" | \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76597d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  ate\n",
      "adjustable  |  adjust\n",
      "rafting  |  raft\n",
      "ability  |  abil\n",
      "meeting  |  meet\n"
     ]
    }
   ],
   "source": [
    "language = 'english'\n",
    "stemmer = SnowballStemmer(language)\n",
    "\n",
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "for word in words:\n",
    "    print(word, \" | \", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a87ae8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating  |  eat\n",
      "eats  |  eat\n",
      "eat  |  eat\n",
      "ate  |  eat\n",
      "adjustable  |  adjustable\n",
      "rafting  |  raft\n",
      "ability  |  ability\n",
      "meeting  |  meeting\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp('eating eats eat ate adjustable rafting ability meeting')\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.lemma_) # token.lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec104d7",
   "metadata": {},
   "source": [
    "# POS vs. Tag\n",
    "#### POS (Part-of-Speech): Refers to the coarse-grained category of a word, NOUN, VERB, ADJ, ADV \n",
    "#### Tag fine-grained category, specific information about the grammatical properties, NN (singular noun), NNS (plural noun), VBZ (verb, 3rd person singular present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adbff4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon  |  PROPN  |  proper noun  |  96\n",
      "Elon  |  NNP  |  noun, proper singular  |  15794550382381185553\n",
      "flew  |  VERB  |  verb  |  100\n",
      "flew  |  VBD  |  verb, past tense  |  17109001835818727656\n",
      "to  |  ADP  |  adposition  |  85\n",
      "to  |  IN  |  conjunction, subordinating or preposition  |  1292078113972184607\n",
      "mars  |  NOUN  |  noun  |  92\n",
      "mars  |  NNS  |  noun, plural  |  783433942507015291\n",
      "yesterday  |  NOUN  |  noun  |  92\n",
      "yesterday  |  NN  |  noun, singular or mass  |  15308085513773655218\n",
      ".  |  PUNCT  |  punctuation  |  97\n",
      ".  |  .  |  punctuation mark, sentence closer  |  12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"Elon flew to mars yesterday.\")\n",
    "for token in doc:\n",
    "    print(token.text, \" | \", token.pos_,\" | \", spacy.explain(token.pos_), \" | \",token.pos)\n",
    "    print(token.text, \" | \", token.tag_,\" | \", spacy.explain(token.tag_), \" | \",token.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d847f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN  |  1\n",
      "VERB  |  1\n",
      "ADP  |  1\n",
      "NOUN  |  2\n",
      "PUNCT  |  1\n"
     ]
    }
   ],
   "source": [
    "count= doc.count_by(spacy.attrs.POS)\n",
    "for a,b in count.items():\n",
    "    print(doc.vocab[a].text, \" | \", b )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b118af",
   "metadata": {},
   "source": [
    "## Name Entity Recognition (NER) for Person, Company, Product, Location, Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a9d25659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "#nlp.pipe_names\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "14d1b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter  |  PRODUCT  |  Objects, vehicles, foods, etc. (not services)\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1388d",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f635e529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9ebc9a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "just\n",
      "our\n",
      "the\n",
      "part\n",
      "is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'opened wings flying coming soon'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"We just opened our wings, the flying part is coming soon\")\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        print(token)\n",
    "\n",
    "\n",
    "filter=[token.text for token in doc \n",
    "        if not token.is_stop and not token.is_punct and not token.text in [\"\\n\",\" \"]]\n",
    "final_doc=\" \".join(filter)\n",
    "final_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4b7d848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize stop words in the text\n",
    "nlp.vocab['part'].is_stop = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
